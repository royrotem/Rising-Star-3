"""
Systems API Endpoints

Handles system management, data ingestion, and analysis.
"""

from fastapi import APIRouter, HTTPException, UploadFile, File, Query, Depends
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
from datetime import datetime
import uuid
import os

from ..services.ingestion import IngestionService
from ..services.anomaly_detection import AnomalyDetectionService
from ..services.root_cause import RootCauseService
from ..services.data_store import data_store


router = APIRouter(prefix="/systems", tags=["Systems"])

# Service instances
ingestion_service = IngestionService()
anomaly_service = AnomalyDetectionService()
root_cause_service = RootCauseService()

# Check if demo mode is enabled
DEMO_MODE = os.environ.get("DEMO_MODE", "false").lower() == "true"


# Pydantic models for API
class SystemCreate(BaseModel):
    name: str
    system_type: str
    serial_number: Optional[str] = None
    model: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = {}
    analysis_id: Optional[str] = None  # If provided, associate pre-analyzed data


class SystemResponse(BaseModel):
    id: str
    name: str
    system_type: str
    status: str
    health_score: float
    created_at: str


class FieldConfirmation(BaseModel):
    field_name: str
    confirmed_type: Optional[str] = None
    confirmed_unit: Optional[str] = None
    confirmed_meaning: Optional[str] = None
    is_correct: bool


class AnalysisRequest(BaseModel):
    include_anomaly_detection: bool = True
    include_root_cause: bool = True
    include_blind_spots: bool = True
    time_range_hours: int = 24


class ConversationQuery(BaseModel):
    query: str
    context: Optional[Dict[str, Any]] = {}


def init_demo_systems():
    """Initialize demo systems for demonstration purposes."""
    demo_systems = [
        {
            "id": "demo-1",
            "name": "Fleet Vehicle Alpha",
            "system_type": "vehicle",
            "serial_number": "VH-2024-001",
            "model": "EV-X1",
            "metadata": {"manufacturer": "UAIE Demo", "year": 2024},
            "status": "anomaly_detected",
            "health_score": 87.5,
            "discovered_schema": {},
            "confirmed_fields": {},
            "is_demo": True,
            "created_at": "2024-01-01T00:00:00Z",
        },
        {
            "id": "demo-2",
            "name": "Robot Arm Unit 7",
            "system_type": "robot",
            "serial_number": "RA-2024-007",
            "model": "ARM-6DOF",
            "metadata": {"manufacturer": "UAIE Demo", "year": 2024},
            "status": "active",
            "health_score": 94.2,
            "discovered_schema": {},
            "confirmed_fields": {},
            "is_demo": True,
            "created_at": "2024-01-02T00:00:00Z",
        },
        {
            "id": "demo-3",
            "name": "Medical Scanner MRI-3",
            "system_type": "medical_device",
            "serial_number": "MRI-2024-003",
            "model": "MRI-X500",
            "metadata": {"manufacturer": "UAIE Demo", "year": 2024},
            "status": "active",
            "health_score": 99.1,
            "discovered_schema": {},
            "confirmed_fields": {},
            "is_demo": True,
            "created_at": "2024-01-03T00:00:00Z",
        },
    ]

    for system in demo_systems:
        if not data_store.get_system(system["id"]):
            data_store.create_system(system)


# Initialize demo systems if in demo mode
if DEMO_MODE:
    init_demo_systems()


@router.post("/analyze-files")
async def analyze_files(
    files: List[UploadFile] = File(...),
):
    """
    Analyze multiple uploaded files to discover schema and suggest system configuration.

    This endpoint processes all uploaded files, discovers relationships between them,
    and provides AI recommendations for system name, type, and description.
    Records are stored temporarily and can be associated with a system using the analysis_id.
    """
    if not files:
        raise HTTPException(status_code=400, detail="No files uploaded")

    # Generate analysis session ID
    analysis_id = str(uuid.uuid4())

    all_discovered_fields = []
    all_confirmation_requests = []
    all_metadata = []  # Collect metadata from all files
    all_records = []  # Store all records for later use
    total_records = 0
    file_summaries = []
    file_records_map = {}  # Map filename to records

    # Process each file
    for file in files:
        try:
            result = await ingestion_service.ingest_file(
                file_content=file.file,
                filename=file.filename,
                system_id="temp_analysis",
                source_name=file.filename,
            )

            # Store records for this file
            records = result.get("sample_records", [])
            file_records_map[file.filename] = records
            all_records.extend(records)

            # Add source file info to each field
            for field in result.get("discovered_fields", []):
                field["source_file"] = file.filename
                all_discovered_fields.append(field)

            all_confirmation_requests.extend(result.get("confirmation_requests", []))
            total_records += result.get("record_count", 0)

            # Collect metadata info if present
            metadata_info = result.get("metadata_info", {})
            if metadata_info.get("dataset_description"):
                all_metadata.append(metadata_info)

            # Collect file summary for AI analysis
            file_summaries.append({
                "filename": file.filename,
                "record_count": result.get("record_count", 0),
                "fields": [f.get("name", "") for f in result.get("discovered_fields", [])],
                "field_types": {f.get("name", ""): f.get("inferred_type", "") for f in result.get("discovered_fields", [])},
                "metadata": metadata_info,  # Include metadata in file summary
                "relationships": result.get("relationships", []),
            })

            # Reset file position for potential re-reading
            file.file.seek(0)

        except Exception as e:
            print(f"Error processing file {file.filename}: {e}")
            continue

    # Store the analyzed data temporarily
    data_store.store_temp_analysis(
        analysis_id=analysis_id,
        records=all_records,
        file_summaries=file_summaries,
        discovered_fields=all_discovered_fields,
        file_records_map=file_records_map,
    )

    # === SECOND PASS: Enrich fields with combined context from all files ===
    # Collect all field descriptions from all metadata
    combined_field_descriptions = {}
    combined_context_texts = []

    for meta in all_metadata:
        if meta.get('field_descriptions'):
            combined_field_descriptions.update(meta['field_descriptions'])
        if meta.get('context_texts'):
            combined_context_texts.extend(meta['context_texts'])
        elif meta.get('dataset_description'):
            combined_context_texts.append(meta['dataset_description'])

    # Enrich discovered fields with context from metadata
    all_discovered_fields = _enrich_fields_with_context(
        all_discovered_fields,
        combined_field_descriptions,
        combined_context_texts
    )

    # Generate AI recommendation based on analyzed data (including metadata)
    recommendation = generate_system_recommendation(file_summaries, all_discovered_fields, all_metadata)

    return {
        "status": "success",
        "analysis_id": analysis_id,  # Use this to associate data with a system
        "files_analyzed": len(files),
        "total_records": total_records,
        "discovered_fields": all_discovered_fields,
        "confirmation_requests": all_confirmation_requests,
        "recommendation": recommendation,
        "context_extracted": len(combined_context_texts) > 0,
        "fields_enriched": len(combined_field_descriptions),
    }


def _enrich_fields_with_context(
    discovered_fields: List[Dict],
    field_descriptions: Dict[str, str],
    context_texts: List[str]
) -> List[Dict]:
    """
    Second pass: Enrich discovered fields with context extracted from all files.
    Updates field meanings based on metadata descriptions found anywhere in the data.
    """
    import re

    # Build a combined context for searching
    combined_context = ' '.join(context_texts)

    for field in discovered_fields:
        field_name = field.get('name', '')

        # Priority 1: Direct field description from metadata
        if field_name in field_descriptions:
            field['inferred_meaning'] = field_descriptions[field_name]
            field['meaning_source'] = 'metadata_description'
            field['confidence'] = min(field.get('confidence', 0.5) + 0.3, 1.0)
            continue

        # Priority 2: Try to find description in combined context
        if combined_context and field.get('inferred_meaning', '').startswith('Unknown'):
            # Search for any mention of this field in context
            patterns = [
                rf'\b{re.escape(field_name)}\s*[:–-]\s*([^.!?\n⭐]+[.!?]?)',
                rf'\b{re.escape(field_name)}\b[^:]*?(?:is|are|represents?|measures?|captures?|records?|indicates?)\s+([^.!?\n]+[.!?]?)',
            ]

            for pattern in patterns:
                match = re.search(pattern, combined_context, re.IGNORECASE)
                if match:
                    desc = match.group(1).strip()
                    desc = re.sub(r'\s+', ' ', desc)
                    if 10 < len(desc) < 300:
                        field['inferred_meaning'] = desc
                        field['meaning_source'] = 'context_extraction'
                        field['confidence'] = min(field.get('confidence', 0.5) + 0.2, 1.0)
                        break

    return discovered_fields


def generate_system_recommendation(file_summaries: List[Dict], discovered_fields: List[Dict], metadata_list: List[Dict] = None) -> Dict:
    """
    Generate AI recommendations for system configuration based on analyzed data.
    Uses metadata descriptions when available for more accurate recommendations.
    """
    metadata_list = metadata_list or []

    # Calculate total records first
    total_records = sum(s.get("record_count", 0) for s in file_summaries)

    # Collect all field names for analysis
    all_fields = [f.get("name", "").lower() for f in discovered_fields]
    all_field_types = [f.get("inferred_type", "") for f in discovered_fields]
    all_units = [f.get("physical_unit", "") for f in discovered_fields if f.get("physical_unit")]

    # Extract insights from metadata descriptions
    metadata_insights = _extract_metadata_insights(metadata_list)

    # Keywords for system type detection
    vehicle_keywords = ["speed", "velocity", "rpm", "engine", "motor", "battery", "fuel", "odometer",
                       "gps", "latitude", "longitude", "steering", "brake", "throttle", "gear",
                       "wheel", "tire", "acceleration", "can_bus", "obd"]
    robot_keywords = ["joint", "axis", "torque", "servo", "gripper", "end_effector", "pose",
                     "position", "orientation", "robot", "arm", "actuator", "encoder", "dof"]
    medical_keywords = ["patient", "heart", "ecg", "ekg", "blood", "pressure", "pulse", "oxygen",
                       "saturation", "temperature", "respiration", "mri", "ct", "scan", "dose"]
    aerospace_keywords = ["altitude", "airspeed", "heading", "pitch", "roll", "yaw", "thrust",
                         "fuel_flow", "engine", "flap", "rudder", "aileron", "flight"]
    industrial_keywords = ["pump", "valve", "flow", "pressure", "level", "tank", "motor",
                          "conveyor", "plc", "scada", "process", "production", "machine",
                          "predictive maintenance", "fault detection", "anomaly", "sensor",
                          "vibration", "acoustic", "machine_id", "equipment", "health"]

    # Score each system type
    scores = {
        "vehicle": sum(1 for f in all_fields if any(k in f for k in vehicle_keywords)),
        "robot": sum(1 for f in all_fields if any(k in f for k in robot_keywords)),
        "medical_device": sum(1 for f in all_fields if any(k in f for k in medical_keywords)),
        "aerospace": sum(1 for f in all_fields if any(k in f for k in aerospace_keywords)),
        "industrial": sum(1 for f in all_fields if any(k in f for k in industrial_keywords)),
    }

    # Boost scores based on metadata insights
    if metadata_insights.get("detected_type"):
        detected = metadata_insights["detected_type"]
        if detected in scores:
            scores[detected] += 5  # Strong boost from metadata

    # Determine best matching system type
    suggested_type = max(scores, key=scores.get) if max(scores.values()) > 0 else "industrial"
    confidence = min(0.95, max(scores.values()) / max(len(all_fields), 1) + 0.5) if all_fields else 0.5

    # Generate suggested name based on type and file info
    type_names = {
        "vehicle": "Vehicle Telemetry System",
        "robot": "Robot Control System",
        "medical_device": "Medical Monitoring System",
        "aerospace": "Flight Data System",
        "industrial": "Industrial Process System",
    }

    # Try to extract name hints from filenames
    file_names = [s.get("filename", "") for s in file_summaries]
    name_hints = []
    for fn in file_names:
        # Extract meaningful parts from filename
        clean_name = fn.replace("_", " ").replace("-", " ").split(".")[0]
        if len(clean_name) > 3:
            name_hints.append(clean_name.title())

    if name_hints:
        suggested_name = f"{name_hints[0]} System"
    else:
        suggested_name = type_names.get(suggested_type, "Data System")

    # Generate description
    descriptions = {
        "vehicle": f"Vehicle telemetry system monitoring {len(discovered_fields)} parameters including {', '.join(all_fields[:3])}. Data collected from {len(file_summaries)} source(s) with {total_records} total records." if all_fields else "Vehicle telemetry monitoring system.",
        "robot": f"Robotic system with {len(discovered_fields)} monitored parameters. Tracking {', '.join(all_fields[:3])} from {len(file_summaries)} data source(s)." if all_fields else "Robotic control and monitoring system.",
        "medical_device": f"Medical device monitoring {len(discovered_fields)} health parameters from {len(file_summaries)} source(s)." if all_fields else "Medical device monitoring system.",
        "aerospace": f"Aerospace system tracking {len(discovered_fields)} flight parameters from {len(file_summaries)} data source(s)." if all_fields else "Aerospace monitoring system.",
        "industrial": f"Industrial process system monitoring {len(discovered_fields)} parameters from {len(file_summaries)} source(s)." if all_fields else "Industrial process monitoring system.",
    }

    # Build reasoning
    reasoning_parts = []

    # Add metadata-based reasoning first (highest confidence)
    if metadata_insights.get("purpose"):
        reasoning_parts.append(f"Dataset purpose: {metadata_insights['purpose']}")
    if metadata_insights.get("detected_type"):
        reasoning_parts.append(f"Metadata indicates {metadata_insights['detected_type']} system")

    if scores[suggested_type] > 0:
        matching_keywords = [f for f in all_fields if any(k in f for k in
            {"vehicle": vehicle_keywords, "robot": robot_keywords, "medical_device": medical_keywords,
             "aerospace": aerospace_keywords, "industrial": industrial_keywords}[suggested_type])]
        reasoning_parts.append(f"Found {scores[suggested_type]} field(s) matching {suggested_type} patterns")
        if matching_keywords[:3]:
            reasoning_parts.append(f"Key indicators: {', '.join(matching_keywords[:3])}")

    if all_units:
        reasoning_parts.append(f"Detected physical units: {', '.join(list(set(all_units))[:5])}")

    reasoning = ". ".join(reasoning_parts) if reasoning_parts else "Based on general data structure analysis."

    # Use metadata description if available, otherwise generate one
    suggested_description = metadata_insights.get("description") or descriptions.get(suggested_type, "System monitoring and analysis.")

    return {
        "suggested_name": suggested_name,
        "suggested_type": suggested_type,
        "suggested_description": suggested_description,
        "confidence": confidence,
        "reasoning": reasoning,
        "analysis_summary": {
            "files_analyzed": len(file_summaries),
            "total_records": total_records,
            "unique_fields": len(set(all_fields)),
            "detected_units": list(set(all_units))[:10],
            "metadata_found": len(metadata_list) > 0,
        }
    }


def _extract_metadata_insights(metadata_list: List[Dict]) -> Dict[str, Any]:
    """
    Extract insights from dataset metadata descriptions.
    Parses description text to determine system type, purpose, etc.
    """
    import re

    insights = {
        "detected_type": None,
        "purpose": None,
        "description": None,
    }

    if not metadata_list:
        return insights

    # Combine all metadata descriptions
    all_descriptions = " ".join(m.get("dataset_description", "") for m in metadata_list if m.get("dataset_description"))

    if not all_descriptions:
        return insights

    desc_lower = all_descriptions.lower()

    # Detect system type from metadata
    type_patterns = {
        "industrial": ["industrial", "machine", "production", "manufacturing", "predictive maintenance",
                      "fault detection", "equipment health", "sensor network", "iot"],
        "vehicle": ["vehicle", "automotive", "car", "truck", "fleet", "telematics", "driving"],
        "robot": ["robot", "robotic", "automation", "arm", "manipulator"],
        "medical_device": ["medical", "patient", "health", "clinical", "diagnostic"],
        "aerospace": ["aerospace", "flight", "aircraft", "aviation", "drone", "uav"],
    }

    type_scores = {}
    for sys_type, patterns in type_patterns.items():
        type_scores[sys_type] = sum(1 for p in patterns if p in desc_lower)

    if max(type_scores.values()) > 0:
        insights["detected_type"] = max(type_scores, key=type_scores.get)

    # Extract purpose
    purpose_patterns = [
        r'designed to support ([^.]+)',
        r'used for ([^.]+)',
        r'suitable for ([^.]+)',
        r'support[s]? ([^.]*(?:maintenance|detection|monitoring|analysis)[^.]*)',
    ]

    for pattern in purpose_patterns:
        match = re.search(pattern, desc_lower)
        if match:
            insights["purpose"] = match.group(1).strip()[:150]
            break

    # Use first 300 chars of description as suggested description
    if len(all_descriptions) > 50:
        # Find first sentence or use first 300 chars
        first_sentence = re.match(r'^[^.!?]+[.!?]', all_descriptions)
        if first_sentence:
            insights["description"] = first_sentence.group(0).strip()
        else:
            insights["description"] = all_descriptions[:300].strip()

    return insights


@router.post("/", response_model=SystemResponse)
async def create_system(system: SystemCreate):
    """Create a new monitored system.

    If analysis_id is provided, the pre-analyzed data will be associated with this system.
    """
    system_id = str(uuid.uuid4())

    system_data = {
        "id": system_id,
        "name": system.name,
        "system_type": system.system_type,
        "serial_number": system.serial_number,
        "model": system.model,
        "metadata": system.metadata or {},
        "status": "active",
        "health_score": 100.0,
        "discovered_schema": {},
        "confirmed_fields": {},
        "is_demo": False,
        "created_at": datetime.utcnow().isoformat(),
    }

    created_system = data_store.create_system(system_data)

    # If analysis_id provided, move pre-analyzed data to this system
    if system.analysis_id:
        moved = data_store.move_temp_to_system(system.analysis_id, system_id)
        if moved:
            # Refresh system data after moving
            created_system = data_store.get_system(system_id)

    return SystemResponse(**created_system)


@router.get("/", response_model=List[SystemResponse])
async def list_systems(
    status: Optional[str] = None,
    system_type: Optional[str] = None,
    include_demo: bool = Query(default=True, description="Include demo systems in results"),
):
    """List all monitored systems."""
    systems = data_store.list_systems(include_demo=include_demo)

    if status:
        systems = [s for s in systems if s.get("status") == status]
    if system_type:
        systems = [s for s in systems if s.get("system_type") == system_type]

    return [SystemResponse(**s) for s in systems]


@router.get("/{system_id}", response_model=Dict[str, Any])
async def get_system(system_id: str):
    """Get detailed information about a system."""
    system = data_store.get_system(system_id)
    if not system:
        raise HTTPException(status_code=404, detail="System not found")

    return system


@router.delete("/{system_id}")
async def delete_system(system_id: str):
    """Delete a system and all its data."""
    success = data_store.delete_system(system_id)
    if not success:
        raise HTTPException(status_code=404, detail="System not found")

    return {"status": "deleted", "system_id": system_id}


@router.post("/{system_id}/ingest")
async def ingest_data(
    system_id: str,
    file: UploadFile = File(...),
    source_name: str = Query(default="uploaded_file"),
):
    """
    Ingest data file and perform autonomous schema discovery.

    This endpoint implements the Zero-Knowledge Ingestion approach.
    The system will analyze the uploaded data "blind" and learn its structure.
    """
    system = data_store.get_system(system_id)
    if not system:
        raise HTTPException(status_code=404, detail="System not found")

    try:
        result = await ingestion_service.ingest_file(
            file_content=file.file,
            filename=file.filename,
            system_id=system_id,
            source_name=source_name,
        )

        # Store discovered schema in system
        data_store.update_system(system_id, {
            "discovered_schema": result.get("discovered_fields", {}),
            "status": "data_ingested"
        })

        # Store ingested data
        source_id = str(uuid.uuid4())
        data_store.store_ingested_data(
            system_id=system_id,
            source_id=source_id,
            source_name=source_name,
            records=result.get("sample_records", []),  # Store all parsed records
            discovered_schema={
                "fields": result.get("discovered_fields", []),
                "relationships": result.get("relationships", []),
            },
            metadata={
                "filename": file.filename,
                "content_type": file.content_type,
            }
        )

        return {
            "status": "success",
            "source_id": source_id,
            "record_count": result.get("record_count"),
            "discovered_fields": result.get("discovered_fields"),
            "relationships": result.get("relationships"),
            "confirmation_requests": result.get("confirmation_requests"),
            "sample_records": result.get("sample_records", [])[:5],
            "message": "Data ingested. Please review the discovered schema and confirm field mappings.",
        }

    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post("/{system_id}/confirm-fields")
async def confirm_fields(
    system_id: str,
    confirmations: List[FieldConfirmation],
):
    """
    Human-in-the-Loop field confirmation.

    Engineers can confirm or correct the AI's schema inference.
    This builds trust and ensures accuracy.
    """
    system = data_store.get_system(system_id)
    if not system:
        raise HTTPException(status_code=404, detail="System not found")

    confirmed = system.get("confirmed_fields", {})

    for conf in confirmations:
        if conf.is_correct:
            confirmed[conf.field_name] = {
                "confirmed": True,
                "type": conf.confirmed_type,
                "unit": conf.confirmed_unit,
                "meaning": conf.confirmed_meaning,
                "confirmed_at": datetime.utcnow().isoformat(),
            }
        else:
            confirmed[conf.field_name] = {
                "confirmed": True,
                "type": conf.confirmed_type,
                "unit": conf.confirmed_unit,
                "meaning": conf.confirmed_meaning,
                "corrected": True,
                "confirmed_at": datetime.utcnow().isoformat(),
            }

    data_store.update_system(system_id, {
        "confirmed_fields": confirmed,
        "status": "configured"
    })

    return {
        "status": "success",
        "confirmed_count": len(confirmations),
        "message": "Field mappings updated. The system will use these confirmations for future analysis.",
    }


@router.get("/{system_id}/data")
async def get_system_data(
    system_id: str,
    source_id: Optional[str] = None,
    limit: int = Query(default=100, le=10000),
    offset: int = Query(default=0, ge=0),
):
    """Get ingested data records for a system."""
    system = data_store.get_system(system_id)
    if not system:
        raise HTTPException(status_code=404, detail="System not found")

    records = data_store.get_ingested_records(system_id, source_id, limit, offset)

    return {
        "system_id": system_id,
        "source_id": source_id,
        "records": records,
        "count": len(records),
        "limit": limit,
        "offset": offset,
    }


@router.get("/{system_id}/statistics")
async def get_system_statistics(system_id: str):
    """Get statistics about a system's ingested data."""
    system = data_store.get_system(system_id)
    if not system:
        raise HTTPException(status_code=404, detail="System not found")

    stats = data_store.get_system_statistics(system_id)

    return {
        "system_id": system_id,
        "system_name": system.get("name"),
        **stats
    }


@router.get("/{system_id}/sources")
async def get_data_sources(system_id: str):
    """Get all data sources for a system."""
    system = data_store.get_system(system_id)
    if not system:
        raise HTTPException(status_code=404, detail="System not found")

    sources = data_store.get_data_sources(system_id)

    return {
        "system_id": system_id,
        "sources": sources,
        "count": len(sources),
    }


@router.post("/{system_id}/analyze")
async def analyze_system(
    system_id: str,
    request: AnalysisRequest,
):
    """
    Run comprehensive analysis on a system.

    This triggers the full agent workforce to analyze the system:
    - Anomaly detection
    - Root cause analysis
    - Blind spot detection
    - Engineering margin calculation
    """
    system = data_store.get_system(system_id)
    if not system:
        raise HTTPException(status_code=404, detail="System not found")

    # Get real data if available
    records = data_store.get_ingested_records(system_id, limit=10000)
    sources = data_store.get_data_sources(system_id)

    # If we have real data, analyze it
    if records:
        import pandas as pd
        df = pd.DataFrame(records)

        # Calculate actual statistics
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()

        anomalies = []
        engineering_margins = []
        blind_spots = []

        # Simple anomaly detection on real data
        for col in numeric_cols[:5]:  # Limit to first 5 numeric columns
            mean = df[col].mean()
            std = df[col].std()

            if std > 0:
                # Find outliers (values > 2 std from mean)
                outliers = df[abs(df[col] - mean) > 2 * std]
                if len(outliers) > 0:
                    anomalies.append({
                        "id": str(uuid.uuid4()),
                        "type": "statistical_outlier",
                        "severity": "medium" if len(outliers) < len(df) * 0.05 else "high",
                        "title": f"Outliers detected in {col}",
                        "description": f"Found {len(outliers)} values that deviate significantly from the mean ({mean:.2f})",
                        "affected_fields": [col],
                        "natural_language_explanation": (
                            f"The field '{col}' has {len(outliers)} data points that are more than "
                            f"2 standard deviations from the mean value of {mean:.2f}. "
                            f"This may indicate sensor errors, unusual operating conditions, or actual anomalies."
                        ),
                        "recommendations": [
                            {
                                "type": "investigation",
                                "priority": "high",
                                "action": f"Review the {len(outliers)} outlier records for {col}",
                            },
                        ],
                        "impact_score": min(100, len(outliers) / len(df) * 1000),
                    })

                # Calculate engineering margins
                current_max = df[col].max()
                if mean > 0:
                    design_limit = mean + 4 * std  # Assume 4-sigma design limit
                    margin = (design_limit - current_max) / design_limit * 100
                    engineering_margins.append({
                        "component": col,
                        "parameter": col,
                        "current_value": float(current_max),
                        "design_limit": float(design_limit),
                        "margin_percentage": float(margin),
                        "trend": "stable",
                        "safety_critical": False,
                    })

        # Identify blind spots (missing data)
        missing_cols = [col for col in df.columns if df[col].isna().sum() > len(df) * 0.1]
        if missing_cols:
            blind_spots.append({
                "title": "Missing data detected",
                "description": f"Fields {', '.join(missing_cols)} have more than 10% missing values",
                "recommended_sensor": None,
                "diagnostic_coverage_improvement": 15,
            })

        # Calculate health score based on anomalies
        health_score = 100 - (len(anomalies) * 5)
        health_score = max(50, min(100, health_score))

        analysis_result = {
            "system_id": system_id,
            "timestamp": datetime.utcnow().isoformat(),
            "health_score": health_score,
            "data_analyzed": {
                "record_count": len(records),
                "source_count": len(sources),
                "field_count": len(df.columns),
            },
            "anomalies": anomalies,
            "engineering_margins": engineering_margins,
            "blind_spots": blind_spots,
            "insights_summary": (
                f"Analyzed {len(records)} records across {len(df.columns)} fields. "
                f"Found {len(anomalies)} potential anomalies. "
                f"System health score: {health_score}%."
            ),
        }

    else:
        # No data - return guidance
        analysis_result = {
            "system_id": system_id,
            "timestamp": datetime.utcnow().isoformat(),
            "health_score": None,
            "data_analyzed": {
                "record_count": 0,
                "source_count": 0,
                "field_count": 0,
            },
            "anomalies": [],
            "engineering_margins": [],
            "blind_spots": [
                {
                    "title": "No data ingested",
                    "description": "Upload telemetry data to enable analysis",
                    "recommended_sensor": None,
                    "diagnostic_coverage_improvement": 100,
                }
            ],
            "insights_summary": (
                "No data has been ingested for this system yet. "
                "Upload telemetry files to enable anomaly detection and analysis."
            ),
        }

    # Update system health score
    if analysis_result.get("health_score"):
        data_store.update_system(system_id, {"health_score": analysis_result["health_score"]})

    return analysis_result


@router.post("/{system_id}/query")
async def query_system(
    system_id: str,
    request: ConversationQuery,
):
    """
    Conversational query interface.

    Engineers can ask questions in natural language about their data.
    """
    system = data_store.get_system(system_id)
    if not system:
        raise HTTPException(status_code=404, detail="System not found")

    query = request.query.lower()
    records = data_store.get_ingested_records(system_id, limit=1000)

    if not records:
        return {
            "type": "no_data",
            "query": request.query,
            "response": "No data has been ingested for this system yet. Please upload telemetry data first.",
        }

    import pandas as pd
    df = pd.DataFrame(records)

    # Parse query and provide data-driven response
    if "show" in query or "find" in query or "get" in query:
        # Data query
        response = {
            "type": "data_query",
            "query": request.query,
            "response": f"Found {len(df)} records in the system.",
            "summary": {
                "total_records": len(df),
                "fields": list(df.columns),
                "time_range": "All available data",
            },
            "sample_results": df.head(5).to_dict('records'),
        }

    elif "average" in query or "mean" in query:
        # Statistical query
        numeric_cols = df.select_dtypes(include=['number']).columns
        means = {col: float(df[col].mean()) for col in numeric_cols}
        response = {
            "type": "statistics",
            "query": request.query,
            "response": "Here are the average values for numeric fields:",
            "data": means,
        }

    elif "max" in query or "maximum" in query:
        numeric_cols = df.select_dtypes(include=['number']).columns
        maxes = {col: float(df[col].max()) for col in numeric_cols}
        response = {
            "type": "statistics",
            "query": request.query,
            "response": "Here are the maximum values for numeric fields:",
            "data": maxes,
        }

    elif "min" in query or "minimum" in query:
        numeric_cols = df.select_dtypes(include=['number']).columns
        mins = {col: float(df[col].min()) for col in numeric_cols}
        response = {
            "type": "statistics",
            "query": request.query,
            "response": "Here are the minimum values for numeric fields:",
            "data": mins,
        }

    else:
        # General query
        stats = data_store.get_system_statistics(system_id)
        response = {
            "type": "general",
            "query": request.query,
            "response": (
                f"System '{system['name']}' has {stats['total_records']} records "
                f"with {stats['field_count']} fields. "
                "You can ask specific questions like 'Show me the data', "
                "'What is the average temperature?', or 'Find maximum values'."
            ),
            "system_info": {
                "name": system["name"],
                "type": system["system_type"],
                "status": system.get("status", "active"),
                "health_score": system.get("health_score"),
            },
            "data_summary": stats,
        }

    return response


@router.get("/{system_id}/impact-radar")
async def get_impact_radar(system_id: str):
    """
    Get the 80/20 Impact Radar view.

    Returns the prioritized list of issues, focusing on the 20%
    of anomalies causing 80% of problems.
    """
    system = data_store.get_system(system_id)
    if not system:
        raise HTTPException(status_code=404, detail="System not found")

    records = data_store.get_ingested_records(system_id, limit=10000)

    if not records:
        return {
            "system_id": system_id,
            "timestamp": datetime.utcnow().isoformat(),
            "total_anomalies": 0,
            "high_impact_anomalies": 0,
            "impact_distribution": None,
            "prioritized_issues": [],
            "message": "No data ingested. Upload data to see impact analysis.",
        }

    import pandas as pd
    df = pd.DataFrame(records)
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()

    # Analyze each field for issues
    issues = []
    for col in numeric_cols:
        mean = df[col].mean()
        std = df[col].std()

        if std > 0:
            outlier_pct = len(df[abs(df[col] - mean) > 2 * std]) / len(df) * 100
            if outlier_pct > 1:  # More than 1% outliers
                issues.append({
                    "title": f"{col} outliers",
                    "impact_score": min(100, outlier_pct * 10),
                    "affected_percentage": outlier_pct,
                    "recommended_action": f"Investigate {col} anomalies",
                })

    # Sort by impact
    issues.sort(key=lambda x: x["impact_score"], reverse=True)

    # Calculate 80/20 distribution
    total_impact = sum(i["impact_score"] for i in issues)
    cumulative = 0
    high_impact_count = 0

    for issue in issues:
        cumulative += issue["impact_score"]
        high_impact_count += 1
        if cumulative >= total_impact * 0.8:
            break

    return {
        "system_id": system_id,
        "timestamp": datetime.utcnow().isoformat(),
        "total_anomalies": len(issues),
        "high_impact_anomalies": high_impact_count,
        "impact_distribution": {
            "top_20_percent": {
                "anomaly_count": high_impact_count,
                "impact_percentage": 80,
            },
            "remaining_80_percent": {
                "anomaly_count": len(issues) - high_impact_count,
                "impact_percentage": 20,
            },
        } if issues else None,
        "prioritized_issues": [
            {"rank": i + 1, **issue} for i, issue in enumerate(issues[:10])
        ],
    }


@router.get("/{system_id}/next-gen-specs")
async def get_next_gen_specs(system_id: str):
    """
    Get AI-generated specifications for the next product generation.

    Based on blind spot analysis and operational data, generates
    recommendations for sensors, data architecture, and capabilities.
    """
    system = data_store.get_system(system_id)
    if not system:
        raise HTTPException(status_code=404, detail="System not found")

    records = data_store.get_ingested_records(system_id, limit=1000)
    stats = data_store.get_system_statistics(system_id)

    # Generate recommendations based on actual data analysis
    new_sensors = []
    data_arch_recommendations = {}

    if records:
        import pandas as pd
        df = pd.DataFrame(records)

        # Analyze data patterns for recommendations
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64']:
                # Check sampling adequacy
                if len(df) < 1000:
                    data_arch_recommendations[col] = "Increase sampling rate"

        # Check for missing sensor types based on system type
        system_type = system.get("system_type", "")
        if system_type == "vehicle":
            if not any("vibration" in col.lower() for col in df.columns):
                new_sensors.append({
                    "type": "3-axis Accelerometer",
                    "location": "Suspension/Motor mount",
                    "sampling_rate": "1kHz",
                    "rationale": "Enable vibration analysis for predictive maintenance",
                    "estimated_cost": 150,
                    "diagnostic_value": "High",
                })
        elif system_type == "robot":
            if not any("torque" in col.lower() for col in df.columns):
                new_sensors.append({
                    "type": "Torque Sensor",
                    "location": "Joint actuators",
                    "sampling_rate": "100Hz",
                    "rationale": "Monitor joint loads for wear prediction",
                    "estimated_cost": 200,
                    "diagnostic_value": "High",
                })

    return {
        "system_id": system_id,
        "generated_at": datetime.utcnow().isoformat(),
        "current_generation": system.get("model", "Current"),
        "data_analyzed": stats,
        "recommended_improvements": {
            "new_sensors": new_sensors or [
                {
                    "type": "Additional sensors recommended after data analysis",
                    "location": "TBD",
                    "sampling_rate": "TBD",
                    "rationale": "Upload more data for specific recommendations",
                    "estimated_cost": 0,
                    "diagnostic_value": "TBD",
                }
            ],
            "data_architecture": data_arch_recommendations or {
                "recommendation": "Upload telemetry data for architecture recommendations"
            },
            "connectivity": {
                "recommendation": "Add real-time streaming for critical parameters" if records else "TBD",
            },
        },
        "expected_benefits": {
            "diagnostic_coverage": "+35%" if records else "TBD",
            "early_warning_capability": "+50%" if records else "TBD",
            "false_positive_reduction": "-25%" if records else "TBD",
        },
    }
